import os
import torch
import pandas as pd
import matplotlib.pyplot as plt
from typing import List, Callable
# import metrics
from xmlc.metrics import *

def evaluate(
    output:torch.LongTensor,
    targets:torch.LongTensor,
    metrics:List[Callable[[torch.Tensor, torch.Tensor, int], float]],
    ks:List[int]
) -> pd.DataFrame:
    # compute all scores
    scores = {
        m.__name__: [m(output, targets, k=k) for k in ks]
        for m in metrics
    }
    # build and return dataframe
    return pd.DataFrame(scores, index=ks)


if __name__ == '__main__':

    from argparse import ArgumentParser
    # build argument parser
    parser = ArgumentParser(description="Compute evaluation metrics from model output and targets.")
    parser.add_argument("--model-output", type=str, help="Path to the model output generated by the predict.py script.")
    parser.add_argument("--sparse-targets", type=str, help="Path to the sparse targets generated by the predict.py script.")
    parser.add_argument("--output-dir", type=str, help="Path to the output directory.")
    # parse arguments
    args = parser.parse_args()

    # load model output and sparse targets
    output = torch.load(args.model_output, map_location='cpu')
    targets = torch.load(args.sparse_targets, map_location='cpu')
    
    # evaluate
    ks = list(range(1, 16))
    metrics = [coverage, hits, ndcg, precision, recall, f1_score]
    scores = evaluate(output, targets, metrics, ks)
    # save scores as csv
    scores.to_csv(os.path.join(args.output_dir, "test-scores.csv"))

    # also plot the scores
    fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 6 * len(metrics)), sharex=True)
    for metric, ax in zip(scores.columns, axes):
        # plot metric
        scores[metric].plot(ax=ax)
        # set axes
        ax.set(title=metric, ylabel=metric, xlabel="k")
        ax.grid()
    # save figure
    fig.savefig(os.path.join(args.output_dir, "test-scores.pdf"))

    # print scores
    print(scores)
