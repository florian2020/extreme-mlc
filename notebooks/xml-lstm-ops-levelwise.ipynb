{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee39daed",
   "metadata": {},
   "source": [
    "# AttentionXML for OPS-Code Prediction using LSTM\n",
    "\n",
    "## Requirements\n",
    "This notebook uses the following non-standard python packages:\n",
    "* numpy\n",
    "* pytorch\n",
    "* transformers\n",
    "* treelib\n",
    "* spacy\n",
    "* matplotlib\n",
    "* tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caaea87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import treelib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from itertools import chain\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db914ee7",
   "metadata": {},
   "source": [
    "Import everything needed from the `xmlc` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1919cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add base directory to path\n",
    "if '../' not in os.sys.path:\n",
    "    os.sys.path.insert(0, '../')\n",
    "# import extreme multi label stuff\n",
    "from xmlc.dataset import NamedTensorDataset\n",
    "from xmlc.plt import ProbabilisticLabelTree\n",
    "from xmlc.utils import build_sparse_tensor\n",
    "from xmlc.tree_utils import index_tree\n",
    "from xmlc.trainer import (\n",
    "    LevelTrainer,\n",
    "    TrainingArgs,\n",
    "    InputsAndLabels\n",
    ")\n",
    "from xmlc.modules import (\n",
    "    MLP, \n",
    "    Attention, \n",
    "    MultiHeadAttention, \n",
    "    LabelAttentionClassifier\n",
    ")\n",
    "from xmlc.metrics import (\n",
    "    MetricsTracker,\n",
    "    precision, \n",
    "    coverage, \n",
    "    hits\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577672de",
   "metadata": {},
   "source": [
    "## Paths and Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3494a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/share/gsg_consulting/AttentionXML/data/ops\"\n",
    "fasttext_path = \"/data/share/gsg_consulting/AttentionXML/models/gsg-fasttext\"\n",
    "tmp_dir = \"/data/share/gsg_consulting/AttentionXML/tmp\"\n",
    "output_dir = \"../output/ops-fasttext-only-pretrained-embeddings-spacy-2-levels\"\n",
    "# create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(tmp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b507d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyperparameters\n",
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a71263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device %s!\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b9861",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d15cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(fpath:str):\n",
    "    with open(fpath, \"r\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def load_labels(fpath:str):\n",
    "    return [line.split() for line in load_texts(fpath)]\n",
    "\n",
    "# load training data\n",
    "train_texts = load_texts(os.path.join(data_path, \"train_texts.txt\"))\n",
    "train_labels = load_labels(os.path.join(data_path, \"train_labels.txt\"))\n",
    "assert len(train_texts) == len(train_labels)\n",
    "# load test data\n",
    "eval_texts = load_texts(os.path.join(data_path, \"test_texts.txt\"))\n",
    "eval_labels = load_labels(os.path.join(data_path, \"test_labels.txt\"))\n",
    "assert len(eval_texts) == len(eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa2ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique labels: 2703\n"
     ]
    }
   ],
   "source": [
    "# get a list of all unique labels\n",
    "unique_labels = np.unique(tuple(chain(*train_labels)))\n",
    "print(\"# unique labels:\", len(unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3a415",
   "metadata": {},
   "source": [
    "## Build simple label tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b167ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = treelib.Tree()\n",
    "# add root node\n",
    "root = tree.create_node(\"Root\", \"Root\")\n",
    "# one level label tree\n",
    "for label in unique_labels:\n",
    "    tree.create_node(label, label, parent=root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd709fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth:       1\n",
      "Totel nodes: 2704\n",
      "Inner nodes: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Depth:      \", tree.depth())\n",
    "print(\"Totel nodes:\", len(tree.all_nodes()))\n",
    "print(\"Inner nodes:\", len(tree.all_nodes()) - len(tree.leaves()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b522c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the tree nodes\n",
    "tree = index_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384106d0",
   "metadata": {},
   "source": [
    "Save the label tree to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d7478aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save the tree to disk\n",
    "with open(os.path.join(output_dir, \"label-tree.bin\"), \"wb+\") as f:\n",
    "    pickle.dump(tree, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a7533",
   "metadata": {},
   "source": [
    "## Build the Training and Evaluation Dataset\n",
    "\n",
    "First load the pretrained word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b408c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocab = np.load(os.path.join(fasttext_path, \"vocab.npy\"))\n",
    "embed = np.load(os.path.join(fasttext_path, \"vectors.npy\"))\n",
    "# change special tokens\n",
    "vocab[vocab == \"<SEP>\"] = \"[SEP]\"\n",
    "vocab[vocab == \"<PAD>\"] = \"[PAD]\"\n",
    "vocab[vocab == \"<UNK>\"] = \"[UNK]\"\n",
    "# convert vocab to list\n",
    "vocab = {token.lower(): i for i, token in enumerate(vocab.tolist())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955fb2f",
   "metadata": {},
   "source": [
    "Then build a tokenized based on the vocab of the pretrained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26fb4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get german tokenizer\n",
    "from spacy.lang.de import German\n",
    "# build tokenizer parameters\n",
    "prefixes = German.Defaults.prefixes\n",
    "suffixes = German.Defaults.suffixes\n",
    "infixes = German.Defaults.infixes\n",
    "prefix_search = spacy.util.compile_prefix_regex(prefixes).search if prefixes else None\n",
    "suffix_search = spacy.util.compile_suffix_regex(suffixes).search if suffixes else None\n",
    "infix_finditer = spacy.util.compile_infix_regex(infixes).finditer if infixes else None\n",
    "# add tokenizer exception for special tokens\n",
    "exc = German.Defaults.tokenizer_exceptions\n",
    "exc = spacy.util.update_exc(exc, {\n",
    "    '[SEP]': [{spacy.symbols.ORTH: \"[SEP]\"}]\n",
    "})\n",
    "# create tokenizer\n",
    "tokenizer = spacy.tokenizer.Tokenizer(\n",
    "    vocab=spacy.vocab.Vocab(strings=vocab.keys()),\n",
    "    rules=exc,\n",
    "    prefix_search=prefix_search,\n",
    "    suffix_search=suffix_search,\n",
    "    infix_finditer=infix_finditer,\n",
    "    token_match=German.Defaults.token_match,\n",
    "    url_match=German.Defaults.url_match\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c161cc",
   "metadata": {},
   "source": [
    "Define a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09b82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tokenize(tokenizer, texts):\n",
    "    \"\"\" tokenize all given texts \"\"\" \n",
    "    return [\n",
    "        tuple(map(lambda t: str(t).lower(), tokenizer(text)))\n",
    "        for text in tqdm(texts, \"Tokenizing\")\n",
    "    ]\n",
    "\n",
    "def truncate_pad(tokenized_texts, max_length=256, padding_token=\"[PAD]\".lower()):\n",
    "    # truncate and pad all tokenized texts to match the `max_legth`\n",
    "    return [\n",
    "        tokens[:max_length] + (padding_token,) * max(0, max_length - len(tokens))\n",
    "        for tokens in tokenized_texts\n",
    "    ]\n",
    "def filter_vocab(vocab, embed, tokenized_texts, min_freq=1, max_size=200_000):\n",
    "    # count token occurances and ignore tokens\n",
    "    # that are not in the vocabulary\n",
    "    counter = Counter(chain(*tokenized_texts))\n",
    "    # create filtered vocabulary containing the most frequent words\n",
    "    filtered_vocab = [\n",
    "        word\n",
    "        for word, freq in counter.most_common()\n",
    "        if (freq > min_freq) and (word in vocab)\n",
    "    ]\n",
    "    filtered_vocab = filtered_vocab[:max_size]\n",
    "#     filtered_vocab = counter.most_common(max_size)\n",
    "#     filtered_vocab = [w for w, f in filtered_vocab if f >= min_freq]\n",
    "    # add special tokens\n",
    "    if \"[SEP]\".lower() in filtered_vocab:\n",
    "        filtered_vocab.remove(\"[SEP]\".lower())\n",
    "    if \"[UNK]\".lower() in filtered_vocab:\n",
    "        filtered_vocab.remove(\"[UNK]\".lower())\n",
    "    if \"[PAD]\".lower() in filtered_vocab:\n",
    "        filtered_vocab.remove(\"[PAD]\".lower())\n",
    "    filtered_vocab.insert(0, \"[SEP]\".lower())\n",
    "    filtered_vocab.insert(0, \"[UNK]\".lower())\n",
    "    filtered_vocab.insert(0, \"[PAD]\".lower())\n",
    "    # build embedding matrix for filtered vocab\n",
    "    filtered_embed = [\n",
    "        embed[vocab[token]] if token in vocab else np.random.uniform(-1, 1, size=(embed.shape[1],))\n",
    "        for token in filtered_vocab\n",
    "    ]\n",
    "    filtered_embed = np.stack(filtered_embed, axis=0)\n",
    "    # create mapping for filtered vocab\n",
    "    filtered_vocab = {token: i for i, token in enumerate(filtered_vocab)}\n",
    "    assert len(filtered_vocab) == filtered_embed.shape[0]\n",
    "    # return\n",
    "    return filtered_vocab, filtered_embed\n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokenized_texts):\n",
    "    unk_token_id = vocab[\"[unk]\"]\n",
    "    return [\n",
    "        [vocab.get(t.lower(), unk_token_id) for t in tokens]\n",
    "        for tokens in tokenized_texts\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c7c8a",
   "metadata": {},
   "source": [
    "And finally preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49b501ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data...\n"
     ]
    }
   ],
   "source": [
    "# check in the tmp dir if the preprocessed data is already there\n",
    "data_dump_path = os.path.join(tmp_dir, \"preprocessed-data-only-pretrained-vocab.bin\")\n",
    "if not os.path.isfile(data_dump_path):\n",
    "    # filter the vocabulary based on train texts\n",
    "    tokenized_texts = tokenize(tokenizer, train_texts)\n",
    "    tokenized_texts = truncate_pad(tokenized_texts, max_length=max_length)\n",
    "    filtered_vocab, filtered_embed = filter_vocab(vocab, embed, tokenized_texts)\n",
    "    pad_token_id = filtered_vocab[\"[PAD]\".lower()]\n",
    "    # build train input features\n",
    "    train_input_ids = torch.LongTensor(convert_tokens_to_ids(filtered_vocab, tokenized_texts))\n",
    "    train_input_mask = (train_input_ids != pad_token_id)\n",
    "    # build test input features\n",
    "    tokenized_texts = tokenize(tokenizer, eval_texts)\n",
    "    tokenized_texts = truncate_pad(tokenized_texts, max_length=max_length)\n",
    "    eval_input_ids = torch.LongTensor(convert_tokens_to_ids(filtered_vocab, tokenized_texts))\n",
    "    eval_input_mask = (eval_input_ids != pad_token_id)\n",
    "    # save to disk\n",
    "    torch.save({\n",
    "        \"train-input-ids\": train_input_ids,\n",
    "        \"train-input-mask\": train_input_mask,\n",
    "        \"eval-input-ids\": eval_input_ids,\n",
    "        \"eval-input-mask\": eval_input_mask,\n",
    "        \"train-labels\": train_labels,\n",
    "        \"eval-labels\": eval_labels,\n",
    "        \"vocab\": filtered_vocab,\n",
    "        \"embedding\": filtered_embed\n",
    "    }, data_dump_path)\n",
    "else:\n",
    "    print(\"Loading cached data...\")\n",
    "    # load the data\n",
    "    data = torch.load(data_dump_path)\n",
    "    # gather all the information\n",
    "    train_input_ids = data[\"train-input-ids\"]\n",
    "    train_input_mask = data[\"train-input-mask\"]\n",
    "    eval_input_ids = data[\"eval-input-ids\"]\n",
    "    eval_input_mask = data[\"eval-input-mask\"]\n",
    "    train_labels = data[\"train-labels\"]\n",
    "    eval_labels = data[\"eval-labels\"]\n",
    "    filtered_vocab = data[\"vocab\"]\n",
    "    filtered_embed = data[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f4ddb",
   "metadata": {},
   "source": [
    "Now lets have a look at some very very basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d0c5030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Vocab Size: 314412\n",
      "Reduced Vocab Size: 115168\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Vocab Size:\", len(vocab))\n",
    "print(\"Reduced Vocab Size:\", len(filtered_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a51319a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Unkown Tokens in train texts: 0.2344548249867361\n",
      "#Unkown Tokens in eval texts:  0.23777897808267998\n"
     ]
    }
   ],
   "source": [
    "# compute ratio of unkown tokens in texts\n",
    "unk_token_id = filtered_vocab[\"[UNK]\".lower()]\n",
    "n_train_unk = (train_input_ids == unk_token_id).sum()\n",
    "n_eval_unk = (eval_input_ids == unk_token_id).sum()\n",
    "# print\n",
    "print(\"#Unkown Tokens in train texts:\", n_train_unk.item() / train_input_ids.numel())\n",
    "print(\"#Unkown Tokens in eval texts: \", n_eval_unk.item() / eval_input_ids.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08014c8e",
   "metadata": {},
   "source": [
    "Pack inputs and labels of the same data split together to don't confuse them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfc6a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train and evaluation data containers\n",
    "train_data = InputsAndLabels(\n",
    "    inputs=NamedTensorDataset(input_ids=train_input_ids, input_mask=train_input_mask),\n",
    "    labels=train_labels\n",
    ")\n",
    "eval_data = InputsAndLabels(\n",
    "    inputs=NamedTensorDataset(input_ids=eval_input_ids, input_mask=eval_input_mask),\n",
    "    labels=eval_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a56a2",
   "metadata": {},
   "source": [
    "## Model\n",
    "The model will use the following simple LSTM encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fef2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Basic LSTM Encoder \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "        embed_size:int,\n",
    "        hidden_size:int, \n",
    "        num_layers:int,\n",
    "        vocab_size:int,\n",
    "        padding_idx:int,\n",
    "        emb_init:torch.FloatTensor =None,\n",
    "        dropout:float =0.2\n",
    "    ) -> None:\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        # create embedding\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_size,\n",
    "            padding_idx=padding_idx,\n",
    "            _weight=emb_init if emb_init is not None else None\n",
    "        )\n",
    "        # create lstm encoder\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        # initial hidden and cell states for lstm\n",
    "        self.h0 = nn.Parameter(torch.zeros(num_layers*2, 1, hidden_size))\n",
    "        self.c0 = nn.Parameter(torch.zeros(num_layers*2, 1, hidden_size))\n",
    "                \n",
    "    def forward(self, \n",
    "        input_ids:torch.LongTensor, \n",
    "        input_mask:torch.BoolTensor\n",
    "    ) -> torch.Tensor:\n",
    "        # flatten parameters\n",
    "        self.lstm.flatten_parameters()\n",
    "        # pass through embedding\n",
    "        b, s = input_ids.size()\n",
    "        x = self.embedding.forward(input_ids)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # pack padded sequences\n",
    "        lengths = input_mask.sum(dim=-1).cpu()\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(\n",
    "            input=x, \n",
    "            lengths=lengths, \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        # apply lstm encoder\n",
    "        h0 = self.h0.repeat_interleave(b, dim=1)\n",
    "        c0 = self.c0.repeat_interleave(b, dim=1)\n",
    "        packed_x, _ = self.lstm(packed_x, (h0, c0))\n",
    "        # unpack packed sequences\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=packed_x, \n",
    "            batch_first=True, \n",
    "            padding_value=0,\n",
    "            total_length=s\n",
    "        )\n",
    "        return F.dropout(x, p=self.dropout, training=self.training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f7782",
   "metadata": {},
   "source": [
    "The LSTM-encoded texts are then passed into a Classifier. The combination of both represents the Model used in each level of the `PLT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3859ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    \"\"\" Combination of a LSTM-encoder and a simple attention-based \n",
    "        Multi-label Classifier Module \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_labels):        \n",
    "        super(ClassificationModel, self).__init__()\n",
    "        # initialize encoder\n",
    "        self.enc = LSTMEncoder(\n",
    "            embed_size=500, \n",
    "            hidden_size=256,\n",
    "            num_layers=1,\n",
    "            vocab_size=len(filtered_vocab), \n",
    "            padding_idx=filtered_vocab['[pad]'], \n",
    "            emb_init=torch.from_numpy(filtered_embed).float(),\n",
    "            dropout=0.5\n",
    "        )\n",
    "        # initialize classifier\n",
    "        self.cls = LabelAttentionClassifier(\n",
    "            hidden_size=2*256, # x2 because lstm is bidirectional\n",
    "            num_labels=num_labels,\n",
    "            attention=Attention(),\n",
    "            #   attention=MultiHeadAttention(\n",
    "            #       embed_dim=2*hidden_size,\n",
    "            #       num_heads=16,\n",
    "            #       dropout=dropout\n",
    "            #   ),\n",
    "            classifier=MLP(2*256, 256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, candidates=None, labels=None):\n",
    "        # apply encoder\n",
    "        x = self.enc(input_ids, input_mask)\n",
    "        # pass through classifier and return logits\n",
    "        logits = self.cls(x, input_mask, candidates)\n",
    "        # classifier returns logits and NOT probabilities\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e893a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Trainable Parameters: 60652929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/ndoll/miniconda3/envs/gsg/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# create probabilistic label tree that uses the classification model\n",
    "model = ProbabilisticLabelTree(\n",
    "    tree=tree,\n",
    "    cls_factory=ClassificationModel\n",
    ")\n",
    "# count the number of parameters to optimize during training\n",
    "n_trainable_params = sum((p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"#Trainable Parameters: %i\" % n_trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614691a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Training\n",
    "\n",
    "Before we can actually train the model we specify a `MetricsTracker` that handles all the computation and tracking of evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "886d88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(MetricsTracker):\n",
    "        \n",
    "    def prepare(self, \n",
    "        preds:torch.Tensor, \n",
    "        labels:torch.Tensor\n",
    "    ):\n",
    "        # convert to long tensors\n",
    "        preds = torch.LongTensor(preds)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        # get the maximum label\n",
    "        num_labels = max(\n",
    "            preds.max().item(), \n",
    "            labels.max().item()\n",
    "        ) + 1\n",
    "        # build sparse targets\n",
    "        sparse_targets = build_sparse_tensor(\n",
    "            args=labels,\n",
    "            mask=(labels >= 0),\n",
    "            size=(labels.size(0), num_labels)\n",
    "        )\n",
    "        # return prepared tensors\n",
    "        return preds, sparse_targets\n",
    "\n",
    "    def compute_log_metrics(self, preds, sparse_targets):\n",
    "        return {\n",
    "            # precision @ k\n",
    "            \"P@1\": precision(preds, sparse_targets, k=1),\n",
    "            \"P@2\": precision(preds, sparse_targets, k=2),\n",
    "            \"P@5\": precision(preds, sparse_targets, k=5),\n",
    "            # coverage @ k\n",
    "            \"C@1\": coverage(preds, sparse_targets, k=1),\n",
    "            \"C@2\": coverage(preds, sparse_targets, k=2),\n",
    "            # hits @ k\n",
    "            \"H@1\": hits(preds, sparse_targets, k=1),\n",
    "            \"H@2\": hits(preds, sparse_targets, k=2),\n",
    "        }\n",
    "    \n",
    "    def compute_additional_metrics(self, preds, sparse_targets):\n",
    "        return {\n",
    "            # precision @ k\n",
    "            \"P@3\": precision(preds, sparse_targets, k=3),\n",
    "            \"P@4\": precision(preds, sparse_targets, k=4),\n",
    "#             \"P@5\": precision(preds, sparse_targets, k=5),\n",
    "            # coverage @ k\n",
    "            \"C@3\": coverage(preds, sparse_targets, k=3),\n",
    "            \"C@4\": coverage(preds, sparse_targets, k=4),\n",
    "            \"C@5\": coverage(preds, sparse_targets, k=5),\n",
    "            # hits @ k\n",
    "            \"H@3\": hits(preds, sparse_targets, k=3),\n",
    "            \"H@4\": hits(preds, sparse_targets, k=4),\n",
    "            \"H@5\": hits(preds, sparse_targets, k=5),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd8e02f",
   "metadata": {},
   "source": [
    "Now we can create the trainer for the first level of the probabilistic label tree abd train the corresponding classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cc6a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metrics instance\n",
    "metrics = Metrics()\n",
    "# set training arguments\n",
    "args = TrainingArgs(\n",
    "    # saving\n",
    "    save_interval=5_000,\n",
    "    save_dir=os.path.join(output_dir, \"level-0\"),\n",
    "    # evaluation\n",
    "    eval_interval = 250,\n",
    "    # batch sizes\n",
    "    train_batch_size=128,\n",
    "    eval_batch_size=256,\n",
    "    # pytorch device\n",
    "    device=device,\n",
    "    # training loop\n",
    "    num_steps=10_000\n",
    ")\n",
    "# create trainer for level 0\n",
    "trainer = LevelTrainer(\n",
    "    level=0,\n",
    "    tree=tree,\n",
    "    model=model,\n",
    "    train_data=train_data,\n",
    "    eval_data=eval_data,\n",
    "    num_candidates=len(unique_labels), # always use all labels as candidates\n",
    "    args=args,\n",
    "    topk=1,\n",
    "    metrics=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e681c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e714dc60bb4d27a09f5980654d86ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/ndoll/miniconda3/envs/gsg/lib/python3.8/site-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 250: {'loss': 0.01069, 'eval_loss': 0.00478, 'P@1': 0.111, 'P@2': 0.118, 'P@5': 0.092, 'C@1': 0.051, 'C@2': 0.109, 'H@1': 0.111, 'H@2': 0.154}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500: {'loss': 0.00441, 'eval_loss': 0.00428, 'P@1': 0.16, 'P@2': 0.147, 'P@5': 0.113, 'C@1': 0.074, 'C@2': 0.136, 'H@1': 0.16, 'H@2': 0.191}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 750: {'loss': 0.00397, 'eval_loss': 0.00385, 'P@1': 0.285, 'P@2': 0.234, 'P@5': 0.156, 'C@1': 0.132, 'C@2': 0.217, 'H@1': 0.285, 'H@2': 0.306}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: {'loss': 0.00344, 'eval_loss': 0.00342, 'P@1': 0.42, 'P@2': 0.344, 'P@5': 0.209, 'C@1': 0.195, 'C@2': 0.319, 'H@1': 0.42, 'H@2': 0.45}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1250: {'loss': 0.003, 'eval_loss': 0.00328, 'P@1': 0.492, 'P@2': 0.389, 'P@5': 0.231, 'C@1': 0.228, 'C@2': 0.36, 'H@1': 0.492, 'H@2': 0.508}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500: {'loss': 0.00274, 'eval_loss': 0.00297, 'P@1': 0.534, 'P@2': 0.421, 'P@5': 0.246, 'C@1': 0.248, 'C@2': 0.391, 'H@1': 0.534, 'H@2': 0.55}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1750: {'loss': 0.00256, 'eval_loss': 0.00262, 'P@1': 0.572, 'P@2': 0.441, 'P@5': 0.255, 'C@1': 0.265, 'C@2': 0.409, 'H@1': 0.572, 'H@2': 0.576}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000: {'loss': 0.00244, 'eval_loss': 0.00254, 'P@1': 0.597, 'P@2': 0.458, 'P@5': 0.264, 'C@1': 0.277, 'C@2': 0.424, 'H@1': 0.597, 'H@2': 0.598}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2250: {'loss': 0.00236, 'eval_loss': 0.00242, 'P@1': 0.615, 'P@2': 0.471, 'P@5': 0.27, 'C@1': 0.285, 'C@2': 0.436, 'H@1': 0.615, 'H@2': 0.615}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500: {'loss': 0.00226, 'eval_loss': 0.00248, 'P@1': 0.623, 'P@2': 0.48, 'P@5': 0.274, 'C@1': 0.289, 'C@2': 0.445, 'H@1': 0.623, 'H@2': 0.627}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2750: {'loss': 0.00221, 'eval_loss': 0.00246, 'P@1': 0.641, 'P@2': 0.489, 'P@5': 0.277, 'C@1': 0.297, 'C@2': 0.454, 'H@1': 0.641, 'H@2': 0.639}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000: {'loss': 0.00214, 'eval_loss': 0.00231, 'P@1': 0.649, 'P@2': 0.496, 'P@5': 0.281, 'C@1': 0.301, 'C@2': 0.46, 'H@1': 0.649, 'H@2': 0.648}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3250: {'loss': 0.00212, 'eval_loss': 0.00216, 'P@1': 0.661, 'P@2': 0.504, 'P@5': 0.284, 'C@1': 0.306, 'C@2': 0.467, 'H@1': 0.661, 'H@2': 0.658}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3500: {'loss': 0.00208, 'eval_loss': 0.00215, 'P@1': 0.664, 'P@2': 0.505, 'P@5': 0.284, 'C@1': 0.308, 'C@2': 0.468, 'H@1': 0.664, 'H@2': 0.66}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3750: {'loss': 0.00206, 'eval_loss': 0.00224, 'P@1': 0.673, 'P@2': 0.511, 'P@5': 0.287, 'C@1': 0.312, 'C@2': 0.474, 'H@1': 0.673, 'H@2': 0.668}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000: {'loss': 0.00203, 'eval_loss': 0.00218, 'P@1': 0.683, 'P@2': 0.515, 'P@5': 0.289, 'C@1': 0.316, 'C@2': 0.478, 'H@1': 0.683, 'H@2': 0.673}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4250: {'loss': 0.00202, 'eval_loss': 0.00219, 'P@1': 0.687, 'P@2': 0.518, 'P@5': 0.29, 'C@1': 0.318, 'C@2': 0.48, 'H@1': 0.687, 'H@2': 0.676}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4500: {'loss': 0.00195, 'eval_loss': 0.00212, 'P@1': 0.689, 'P@2': 0.523, 'P@5': 0.292, 'C@1': 0.319, 'C@2': 0.485, 'H@1': 0.689, 'H@2': 0.683}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4750: {'loss': 0.0019, 'eval_loss': 0.00211, 'P@1': 0.699, 'P@2': 0.526, 'P@5': 0.294, 'C@1': 0.324, 'C@2': 0.488, 'H@1': 0.699, 'H@2': 0.687}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000: {'loss': 0.00189, 'eval_loss': 0.00207, 'P@1': 0.7, 'P@2': 0.527, 'P@5': 0.294, 'C@1': 0.324, 'C@2': 0.488, 'H@1': 0.7, 'H@2': 0.688}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5250: {'loss': 0.00188, 'eval_loss': 0.00198, 'P@1': 0.708, 'P@2': 0.532, 'P@5': 0.296, 'C@1': 0.328, 'C@2': 0.494, 'H@1': 0.708, 'H@2': 0.696}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5500: {'loss': 0.00188, 'eval_loss': 0.00214, 'P@1': 0.709, 'P@2': 0.534, 'P@5': 0.298, 'C@1': 0.329, 'C@2': 0.495, 'H@1': 0.709, 'H@2': 0.697}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5750: {'loss': 0.00185, 'eval_loss': 0.00212, 'P@1': 0.708, 'P@2': 0.536, 'P@5': 0.297, 'C@1': 0.328, 'C@2': 0.497, 'H@1': 0.708, 'H@2': 0.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6000: {'loss': 0.00182, 'eval_loss': 0.00208, 'P@1': 0.714, 'P@2': 0.534, 'P@5': 0.297, 'C@1': 0.331, 'C@2': 0.495, 'H@1': 0.714, 'H@2': 0.698}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6250: {'loss': 0.00184, 'eval_loss': 0.00201, 'P@1': 0.714, 'P@2': 0.539, 'P@5': 0.299, 'C@1': 0.331, 'C@2': 0.499, 'H@1': 0.714, 'H@2': 0.704}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6500: {'loss': 0.00182, 'eval_loss': 0.00214, 'P@1': 0.716, 'P@2': 0.541, 'P@5': 0.301, 'C@1': 0.332, 'C@2': 0.501, 'H@1': 0.716, 'H@2': 0.706}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6750: {'loss': 0.00181, 'eval_loss': 0.00198, 'P@1': 0.719, 'P@2': 0.54, 'P@5': 0.3, 'C@1': 0.333, 'C@2': 0.501, 'H@1': 0.719, 'H@2': 0.706}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7000: {'loss': 0.00171, 'eval_loss': 0.00204, 'P@1': 0.723, 'P@2': 0.544, 'P@5': 0.3, 'C@1': 0.335, 'C@2': 0.504, 'H@1': 0.723, 'H@2': 0.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7250: {'loss': 0.00173, 'eval_loss': 0.00204, 'P@1': 0.728, 'P@2': 0.544, 'P@5': 0.302, 'C@1': 0.338, 'C@2': 0.505, 'H@1': 0.728, 'H@2': 0.711}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7500: {'loss': 0.00172, 'eval_loss': 0.00206, 'P@1': 0.729, 'P@2': 0.548, 'P@5': 0.302, 'C@1': 0.338, 'C@2': 0.508, 'H@1': 0.729, 'H@2': 0.715}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7750: {'loss': 0.00174, 'eval_loss': 0.00194, 'P@1': 0.729, 'P@2': 0.547, 'P@5': 0.303, 'C@1': 0.338, 'C@2': 0.507, 'H@1': 0.729, 'H@2': 0.714}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000: {'loss': 0.00171, 'eval_loss': 0.00208, 'P@1': 0.732, 'P@2': 0.548, 'P@5': 0.304, 'C@1': 0.339, 'C@2': 0.508, 'H@1': 0.732, 'H@2': 0.716}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8250: {'loss': 0.0017, 'eval_loss': 0.002, 'P@1': 0.732, 'P@2': 0.55, 'P@5': 0.303, 'C@1': 0.339, 'C@2': 0.51, 'H@1': 0.732, 'H@2': 0.718}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8500: {'loss': 0.00171, 'eval_loss': 0.00202, 'P@1': 0.738, 'P@2': 0.551, 'P@5': 0.305, 'C@1': 0.342, 'C@2': 0.511, 'H@1': 0.738, 'H@2': 0.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8750: {'loss': 0.00173, 'eval_loss': 0.00207, 'P@1': 0.736, 'P@2': 0.552, 'P@5': 0.305, 'C@1': 0.341, 'C@2': 0.512, 'H@1': 0.736, 'H@2': 0.721}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9000: {'loss': 0.00171, 'eval_loss': 0.00195, 'P@1': 0.737, 'P@2': 0.553, 'P@5': 0.305, 'C@1': 0.341, 'C@2': 0.513, 'H@1': 0.737, 'H@2': 0.723}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9250: {'loss': 0.00162, 'eval_loss': 0.00207, 'P@1': 0.741, 'P@2': 0.554, 'P@5': 0.307, 'C@1': 0.343, 'C@2': 0.514, 'H@1': 0.741, 'H@2': 0.724}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ee949",
   "metadata": {},
   "source": [
    "After training is finished we can visualize the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4171fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_p, ax_c, ax_h) = plt.subplots(4, 1, figsize=(12, 20), sharex=True)\n",
    "# plot losses\n",
    "ax_loss.plot(metrics.steps, metrics.loss, label=\"train\")\n",
    "ax_loss.plot(metrics.steps, metrics.eval_loss, label=\"test\")\n",
    "ax_loss.set(\n",
    "    title=\"Train and Test Loss\",\n",
    "    ylabel=\"Loss\"\n",
    ")\n",
    "ax_loss.legend()\n",
    "ax_loss.grid()\n",
    "# plot precision\n",
    "ax_p.plot(metrics.steps, metrics['P@1'], label=\"$k=1$\")\n",
    "ax_p.plot(metrics.steps, metrics['P@2'], label=\"$k=2$\")\n",
    "ax_p.plot(metrics.steps, metrics['P@3'], label=\"$k=3$\")\n",
    "ax_p.plot(metrics.steps, metrics['P@5'], label=\"$k=5$\")\n",
    "ax_p.set(\n",
    "    title=\"Precision @ k\",\n",
    "    ylabel=\"Precision\"\n",
    ")\n",
    "ax_p.legend()\n",
    "ax_p.grid()\n",
    "# plot coverage\n",
    "ax_c.plot(metrics.steps, metrics['C@1'], label=\"$k=1$\")\n",
    "ax_c.plot(metrics.steps, metrics['C@2'], label=\"$k=2$\")\n",
    "ax_c.plot(metrics.steps, metrics['C@3'], label=\"$k=3$\")\n",
    "ax_c.plot(metrics.steps, metrics['C@5'], label=\"$k=5$\")\n",
    "ax_c.set(\n",
    "    title=\"Coverage @ k\",\n",
    "    ylabel=\"Coverage\"\n",
    ")\n",
    "ax_c.legend()\n",
    "ax_c.grid()\n",
    "# plot precision\n",
    "ax_h.plot(metrics.steps, metrics['H@1'], label=\"$k=1$\")\n",
    "ax_h.plot(metrics.steps, metrics['H@2'], label=\"$k=2$\")\n",
    "ax_h.plot(metrics.steps, metrics['H@3'], label=\"$k=3$\")\n",
    "ax_h.plot(metrics.steps, metrics['H@5'], label=\"$k=5$\")\n",
    "ax_h.set(\n",
    "    title=\"Hits @ k\",\n",
    "    ylabel=\"Hits\",\n",
    "    xlabel=\"Global Steps\"\n",
    ")\n",
    "ax_h.legend()\n",
    "ax_h.grid()\n",
    "# save and show\n",
    "fig.savefig(os.path.join(output_dir, \"metrics.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f885a",
   "metadata": {},
   "source": [
    "Save final metric scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c83dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final metrics, i.e. the scores of the very last evaluation step\n",
    "final_metrics = {metric: values[-1] for metric, values in metrics.metrics.items()}\n",
    "# save them to disk\n",
    "with open(os.path.join(output_dir, \"final_scores.json\"), \"w+\") as f:\n",
    "    f.write(json.dumps(final_metrics, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8dbbcf",
   "metadata": {},
   "source": [
    "Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abceb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model state dict to disk\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b78285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
