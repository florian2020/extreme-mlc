{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee39daed",
   "metadata": {},
   "source": [
    "# Toy Example of AttentionXML Model\n",
    "\n",
    "## Requirements\n",
    "This notebook uses the following non-standard python packages:\n",
    "* numpy\n",
    "* pytorch\n",
    "* transformers\n",
    "* treelib\n",
    "* spacy\n",
    "* matplotlib\n",
    "* tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caaea87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import treelib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from itertools import chain\n",
    "from dataclasses import dataclass\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple, List, Set, Callable, Iterator, Iterable, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1919cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add base directory to path\n",
    "if '../' not in os.sys.path:\n",
    "    os.sys.path.insert(0, '../')\n",
    "# import extreme multi label stuff\n",
    "from xmlc.dataset import XMLDataset\n",
    "from xmlc.modules import MLP, Attention, MultiHeadAttention, PLTHierarchy\n",
    "from xmlc.metrics import precision\n",
    "from xmlc.tree_utils import index_tree, convert_labels_to_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577672de",
   "metadata": {},
   "source": [
    "## Paths and Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3494a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/share/gsg_consulting/AttentionXML/data/ops\"\n",
    "fasttext_path = \"/data/share/gsg_consulting/AttentionXML/models/gsg-fasttext\"\n",
    "output_dir = \"../output/ops-fasttext-simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b507d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyperparameters\n",
    "max_length = 512\n",
    "num_candidates = 128\n",
    "# model hyperparameters\n",
    "hidden_size=128\n",
    "num_lstm_layers=1\n",
    "dropout=0.5\n",
    "# training hyperparemers\n",
    "lr = 1e-3\n",
    "num_epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a71263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device %s!\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b9861",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d15cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(fpath:str):\n",
    "    with open(fpath, \"r\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def load_labels(fpath:str):\n",
    "    return [line.split() for line in load_texts(fpath)]\n",
    "\n",
    "# load training data\n",
    "train_texts = load_texts(os.path.join(data_path, \"train_texts.txt\"))\n",
    "train_labels = load_labels(os.path.join(data_path, \"train_labels.txt\"))\n",
    "assert len(train_texts) == len(train_labels)\n",
    "# load test data\n",
    "test_texts = load_texts(os.path.join(data_path, \"test_texts.txt\"))\n",
    "test_labels = load_labels(os.path.join(data_path, \"test_labels.txt\"))\n",
    "assert len(test_texts) == len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa2ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique labels: 2703\n"
     ]
    }
   ],
   "source": [
    "# get a list of all unique labels\n",
    "unique_labels = np.unique(tuple(chain(*train_labels)))\n",
    "print(\"# unique labels:\", len(unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3a415",
   "metadata": {},
   "source": [
    "## Build simple label tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b167ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = treelib.Tree()\n",
    "# add root node\n",
    "root = tree.create_node(\"Root\", \"Root\")\n",
    "# no real hierarchies as all labels are in level 1\n",
    "for label in unique_labels:\n",
    "    tree.create_node(label, label, parent=root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd709fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth:       1\n",
      "Totel nodes: 2704\n",
      "Inner nodes: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Depth:      \", tree.depth())\n",
    "print(\"Totel nodes:\", len(tree.all_nodes()))\n",
    "print(\"Inner nodes:\", len(tree.all_nodes()) - len(tree.leaves()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b522c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the tree nodes\n",
    "tree = index_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a7533",
   "metadata": {},
   "source": [
    "## Build Training and Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b408c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocab = np.load(os.path.join(fasttext_path, \"vocab.npy\"))\n",
    "embed = np.load(os.path.join(fasttext_path, \"vectors.npy\"))\n",
    "# change special tokens\n",
    "vocab[vocab == \"<SEP>\"] = \"[SEP]\"\n",
    "vocab[vocab == \"<PAD>\"] = \"[PAD]\"\n",
    "vocab[vocab == \"<UNK>\"] = \"[UNK]\"\n",
    "# build token-id map\n",
    "vocab_map = {token.lower(): i for i, token in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26fb4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get german tokenizer\n",
    "from spacy.lang.de import German\n",
    "# build tokenizer parameters\n",
    "prefixes = German.Defaults.prefixes\n",
    "suffixes = German.Defaults.suffixes\n",
    "infixes = German.Defaults.infixes\n",
    "prefix_search = spacy.util.compile_prefix_regex(prefixes).search if prefixes else None\n",
    "suffix_search = spacy.util.compile_suffix_regex(suffixes).search if suffixes else None\n",
    "infix_finditer = spacy.util.compile_infix_regex(infixes).finditer if infixes else None\n",
    "# add tokenizer exception for special tokens\n",
    "exc = German.Defaults.tokenizer_exceptions\n",
    "exc = spacy.util.update_exc(exc, {\n",
    "    '[SEP]': [{spacy.symbols.ORTH: \"[SEP]\"}]\n",
    "})\n",
    "# create tokenizer\n",
    "tokenizer = spacy.tokenizer.Tokenizer(\n",
    "    vocab=spacy.vocab.Vocab(strings=vocab.tolist()),\n",
    "    rules=exc,\n",
    "    prefix_search=prefix_search,\n",
    "    suffix_search=suffix_search,\n",
    "    infix_finditer=infix_finditer,\n",
    "    token_match=German.Defaults.token_match,\n",
    "    url_match=German.Defaults.url_match\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e09b82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_features(texts, max_length=256):\n",
    "    # tokenize all texts and convert tokens to ids\n",
    "    input_ids = []\n",
    "    unk_token_id = vocab_map['[unk]']\n",
    "    pad_token_id = vocab_map['[pad]']\n",
    "    for text in tqdm(texts, \"Tokenizing\"):\n",
    "        # tokenize and convert to ids\n",
    "        doc = tokenizer(text)\n",
    "        ids = [vocab_map.get(str(t).lower(), unk_token_id) for t in doc[:max_length]]\n",
    "        ids = ids + [pad_token_id] * max(max_length - len(ids), 0)\n",
    "        # add to list\n",
    "        input_ids.append(ids)\n",
    "    # convert to tensor\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "    input_mask = (input_ids) != pad_token_id\n",
    "    # return features\n",
    "    return input_ids, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e6917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_input_ids, train_input_mask = build_input_features(train_texts, max_length=max_length)\n",
    "# test_input_ids, test_input_mask = build_input_features(test_texts, max_length=max_length)\n",
    "data = torch.load(\"data.bin\")\n",
    "train_input_ids = data[\"train-input-ids\"]\n",
    "train_input_mask = data[\"train-input-mask\"]\n",
    "test_input_ids = data[\"test-input-ids\"]\n",
    "test_input_mask = data[\"test-input-mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ac5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train and test datasets\n",
    "train_data = XMLDataset(\n",
    "    input_dataset=TensorDataset(train_input_ids, train_input_mask),\n",
    "    labels=convert_labels_to_ids(tree, train_labels),\n",
    "    num_candidates=num_candidates\n",
    ")\n",
    "eval_data = XMLDataset(\n",
    "    input_dataset=TensorDataset(test_input_ids, test_input_mask),\n",
    "    labels=convert_labels_to_ids(tree, test_labels),\n",
    "    num_candidates=num_candidates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a56a2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fef2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Basic LSTM Encoder \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "        embed_size:int,\n",
    "        hidden_size:int, \n",
    "        num_layers:int,\n",
    "        vocab_size:int,\n",
    "        padding_idx:int,\n",
    "        emb_init:torch.FloatTensor =None,\n",
    "        dropout:float =0.2\n",
    "    ) -> None:\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        # create embedding\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_size,\n",
    "            padding_idx=padding_idx,\n",
    "            _weight=emb_init if emb_init is not None else None\n",
    "        )\n",
    "        # create lstm encoder\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        # initial hidden and cell states for lstm\n",
    "        self.h0 = nn.Parameter(torch.zeros(num_layers*2, 1, hidden_size))\n",
    "        self.c0 = nn.Parameter(torch.zeros(num_layers*2, 1, hidden_size))\n",
    "                \n",
    "    def forward(self, \n",
    "        input_ids:torch.LongTensor, \n",
    "        input_mask:torch.BoolTensor\n",
    "    ) -> torch.Tensor:\n",
    "        # flatten parameters\n",
    "        self.lstm.flatten_parameters()\n",
    "        # pass through embedding\n",
    "        b, s = input_ids.size()\n",
    "        x = self.embedding.forward(input_ids)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # pack padded sequences\n",
    "        lengths = input_mask.sum(dim=-1).cpu()\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(\n",
    "            input=x, \n",
    "            lengths=lengths, \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        # apply lstm encoder\n",
    "        h0 = self.h0.repeat_interleave(b, dim=1)\n",
    "        c0 = self.c0.repeat_interleave(b, dim=1)\n",
    "        packed_x, _ = self.lstm(packed_x, (h0, c0))\n",
    "        # unpack packed sequences\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=packed_x, \n",
    "            batch_first=True, \n",
    "            padding_value=0,\n",
    "            total_length=s\n",
    "        )\n",
    "        return F.dropout(x, p=self.dropout, training=self.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e092cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\" Model Combining LSTM-Encoder and a PLT-Hierarchy Classifier \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # create encoder\n",
    "        self.enc = LSTMEncoder(\n",
    "            embed_size=500, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            vocab_size=vocab.shape[0], \n",
    "            padding_idx=vocab_map['[pad]'], \n",
    "            emb_init=torch.from_numpy(embed).float(),\n",
    "            dropout=dropout\n",
    "        )\n",
    "        # create hierarchy classifier\n",
    "        self.plt = PLTHierarchy(\n",
    "            hidden_size=2*hidden_size, \n",
    "            num_labels=len(unique_labels),\n",
    "            attention=Attention(dropout=0.5),\n",
    "#             attention=MultiHeadAttention(\n",
    "#                 embed_dim=2*hidden_size,\n",
    "#                 num_heads=16,\n",
    "#                 dropout=0.5\n",
    "#             ),\n",
    "            classifier=MLP(2*hidden_size, 128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, candidates, labels=None):\n",
    "        # pass through encoder\n",
    "        x = self.enc(input_ids, input_mask)\n",
    "        # apply classifier\n",
    "        logits = self.plt(x, input_mask, candidates)\n",
    "        # compute loss if targets given\n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "            return {'loss': loss, 'logits': logits}\n",
    "        # return logits only\n",
    "        return {'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "241b5b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/ndoll/miniconda3/envs/gsg/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Model().to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"Using %i GPUs!\" % torch.cuda.device_count())\n",
    "# create optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e893a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Trainable Parameters: 158576625\n"
     ]
    }
   ],
   "source": [
    "n_trainable_params = sum((p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"#Trainable Parameters: %i\" % n_trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580322dc",
   "metadata": {},
   "source": [
    "## Training\n",
    "I'm way too lazy to write this from scratch so lets just use the transformers Trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04f624cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    # unpack predictions and labels\n",
    "    preds, labels = eval_preds\n",
    "    preds = torch.FloatTensor(preds)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    _, preds = torch.topk(preds, k=100, dim=-1)\n",
    "    # compute metrics\n",
    "    return {\n",
    "        \"P@1\": precision(preds, labels, k=1),\n",
    "        \"P@2\": precision(preds, labels, k=2),\n",
    "        \"P@3\": precision(preds, labels, k=3),\n",
    "        \"P@5\": precision(preds, labels, k=5)\n",
    "    }\n",
    "\n",
    "def collate(batch):\n",
    "    \"\"\" default collate and return as dictionary \"\"\"\n",
    "    return dict(zip(\n",
    "        ('input_ids', 'input_mask', 'candidates', 'labels'),\n",
    "        torch.utils.data._utils.collate.default_collate(batch)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59d89443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=256,\n",
    "    save_steps=5_000,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=250,\n",
    "    eval_steps=250,\n",
    "    evaluation_strategy='steps'\n",
    ")\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    optimizers=(optim, None),\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a48227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 284607\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8896\n",
      "/home/IAIS/ndoll/miniconda3/envs/gsg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8896' max='8896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8896/8896 59:37, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@2</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.131400</td>\n",
       "      <td>0.069999</td>\n",
       "      <td>0.469755</td>\n",
       "      <td>0.396032</td>\n",
       "      <td>0.333112</td>\n",
       "      <td>0.243497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.060807</td>\n",
       "      <td>0.515847</td>\n",
       "      <td>0.420314</td>\n",
       "      <td>0.353132</td>\n",
       "      <td>0.260090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.056927</td>\n",
       "      <td>0.554669</td>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.371410</td>\n",
       "      <td>0.271637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.055700</td>\n",
       "      <td>0.052541</td>\n",
       "      <td>0.592350</td>\n",
       "      <td>0.472559</td>\n",
       "      <td>0.391890</td>\n",
       "      <td>0.288382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.047485</td>\n",
       "      <td>0.615253</td>\n",
       "      <td>0.491518</td>\n",
       "      <td>0.407159</td>\n",
       "      <td>0.298047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.042614</td>\n",
       "      <td>0.652316</td>\n",
       "      <td>0.519363</td>\n",
       "      <td>0.429477</td>\n",
       "      <td>0.311133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.040377</td>\n",
       "      <td>0.693894</td>\n",
       "      <td>0.548919</td>\n",
       "      <td>0.449782</td>\n",
       "      <td>0.323203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.035557</td>\n",
       "      <td>0.723687</td>\n",
       "      <td>0.571870</td>\n",
       "      <td>0.464845</td>\n",
       "      <td>0.331813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.032872</td>\n",
       "      <td>0.751580</td>\n",
       "      <td>0.590140</td>\n",
       "      <td>0.478958</td>\n",
       "      <td>0.338988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.030574</td>\n",
       "      <td>0.770872</td>\n",
       "      <td>0.607365</td>\n",
       "      <td>0.490045</td>\n",
       "      <td>0.344433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.028792</td>\n",
       "      <td>0.791780</td>\n",
       "      <td>0.621573</td>\n",
       "      <td>0.499564</td>\n",
       "      <td>0.349898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.027416</td>\n",
       "      <td>0.804277</td>\n",
       "      <td>0.630387</td>\n",
       "      <td>0.505235</td>\n",
       "      <td>0.353053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.026202</td>\n",
       "      <td>0.818817</td>\n",
       "      <td>0.640532</td>\n",
       "      <td>0.513059</td>\n",
       "      <td>0.356968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.024942</td>\n",
       "      <td>0.831979</td>\n",
       "      <td>0.649442</td>\n",
       "      <td>0.517542</td>\n",
       "      <td>0.359553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.024222</td>\n",
       "      <td>0.838536</td>\n",
       "      <td>0.653766</td>\n",
       "      <td>0.521375</td>\n",
       "      <td>0.362091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.023510</td>\n",
       "      <td>0.846139</td>\n",
       "      <td>0.660513</td>\n",
       "      <td>0.525889</td>\n",
       "      <td>0.363934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.022574</td>\n",
       "      <td>0.855405</td>\n",
       "      <td>0.664956</td>\n",
       "      <td>0.528819</td>\n",
       "      <td>0.365683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.021961</td>\n",
       "      <td>0.859349</td>\n",
       "      <td>0.667332</td>\n",
       "      <td>0.531591</td>\n",
       "      <td>0.367679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.864243</td>\n",
       "      <td>0.672155</td>\n",
       "      <td>0.534141</td>\n",
       "      <td>0.368591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.021149</td>\n",
       "      <td>0.867379</td>\n",
       "      <td>0.674555</td>\n",
       "      <td>0.535519</td>\n",
       "      <td>0.370302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.020640</td>\n",
       "      <td>0.870325</td>\n",
       "      <td>0.675267</td>\n",
       "      <td>0.536216</td>\n",
       "      <td>0.370406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.020180</td>\n",
       "      <td>0.878213</td>\n",
       "      <td>0.678522</td>\n",
       "      <td>0.538734</td>\n",
       "      <td>0.371993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.019950</td>\n",
       "      <td>0.880874</td>\n",
       "      <td>0.681563</td>\n",
       "      <td>0.539907</td>\n",
       "      <td>0.372469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.886624</td>\n",
       "      <td>0.683654</td>\n",
       "      <td>0.541681</td>\n",
       "      <td>0.373362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.019465</td>\n",
       "      <td>0.882918</td>\n",
       "      <td>0.684034</td>\n",
       "      <td>0.542599</td>\n",
       "      <td>0.373742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.887289</td>\n",
       "      <td>0.685507</td>\n",
       "      <td>0.543407</td>\n",
       "      <td>0.374037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.019034</td>\n",
       "      <td>0.891091</td>\n",
       "      <td>0.687218</td>\n",
       "      <td>0.544056</td>\n",
       "      <td>0.375244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.892849</td>\n",
       "      <td>0.688073</td>\n",
       "      <td>0.545197</td>\n",
       "      <td>0.375110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.018748</td>\n",
       "      <td>0.891993</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>0.546179</td>\n",
       "      <td>0.375852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.018601</td>\n",
       "      <td>0.897648</td>\n",
       "      <td>0.691423</td>\n",
       "      <td>0.546971</td>\n",
       "      <td>0.376393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.018403</td>\n",
       "      <td>0.898361</td>\n",
       "      <td>0.691732</td>\n",
       "      <td>0.546765</td>\n",
       "      <td>0.376745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.018261</td>\n",
       "      <td>0.896888</td>\n",
       "      <td>0.691875</td>\n",
       "      <td>0.546591</td>\n",
       "      <td>0.376660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.018221</td>\n",
       "      <td>0.895462</td>\n",
       "      <td>0.692587</td>\n",
       "      <td>0.547573</td>\n",
       "      <td>0.376698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.898266</td>\n",
       "      <td>0.691875</td>\n",
       "      <td>0.548666</td>\n",
       "      <td>0.377572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.018127</td>\n",
       "      <td>0.898123</td>\n",
       "      <td>0.692659</td>\n",
       "      <td>0.548602</td>\n",
       "      <td>0.377553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../output/ops-fasttext-simple/checkpoint-5000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/home/IAIS/ndoll/miniconda3/envs/gsg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8896, training_loss=0.030857581993658765, metrics={'train_runtime': 3580.265, 'train_samples_per_second': 635.946, 'train_steps_per_second': 2.485, 'total_flos': 0.0, 'train_loss': 0.030857581993658765, 'epoch': 8.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aee4ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final model state dict to disk\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"final-model.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a4082",
   "metadata": {},
   "source": [
    "## Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ed386c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only train and test logs\n",
    "train_logs = [log for log in trainer.state.log_history if 'loss' in log]\n",
    "eval_logs = [log for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "# gather values from logs\n",
    "train_metrics = {\n",
    "    key: [log[key] for log in train_logs]\n",
    "    for key in ['step', 'loss']\n",
    "}\n",
    "eval_logs = {\n",
    "    key: [log[key] for log in eval_logs]\n",
    "    for key in [\n",
    "        'step', \n",
    "        'eval_loss', \n",
    "        'eval_P@1', \n",
    "        'eval_P@2', \n",
    "        'eval_P@3', \n",
    "        'eval_P@5'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d770fc0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (507289498.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_4972/507289498.py\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    fig.savefig(os.path.join(output_dir\", \"metrics.pdf\"))\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fig, (ax_loss, ax_p) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "# plot losses\n",
    "ax_loss.plot(train_metrics['step'], train_metrics['loss'], label=\"train\")\n",
    "ax_loss.plot(eval_logs['step'], eval_logs['eval_loss'], label=\"eval\")\n",
    "ax_loss.set(\n",
    "    title=\"Train and Test Loss\",\n",
    "    ylabel=\"Loss\"\n",
    ")\n",
    "ax_loss.legend()\n",
    "ax_loss.grid()\n",
    "# plot f1-scores\n",
    "ax_p.plot(eval_logs['step'], eval_logs['eval_P@1'], label=\"$k=1$\")\n",
    "ax_p.plot(eval_logs['step'], eval_logs['eval_P@2'], label=\"$k=2$\")\n",
    "ax_p.plot(eval_logs['step'], eval_logs['eval_P@3'], label=\"$k=3$\")\n",
    "ax_p.plot(eval_logs['step'], eval_logs['eval_P@5'], label=\"$k=5$\")\n",
    "ax_p.set(\n",
    "    title=\"Precision@k\",\n",
    "    xlabel=\"Global Step\",\n",
    "    ylabel=\"Precision\"\n",
    ")\n",
    "ax_p.legend()\n",
    "ax_p.grid()\n",
    "# save figure\n",
    "fig.savefig(os.path.join(output_dir, \"metrics.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7e586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d691e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
