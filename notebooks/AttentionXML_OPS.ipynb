{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee39daed",
   "metadata": {},
   "source": [
    "# Toy Example of AttentionXML Model\n",
    "\n",
    "## Requirements\n",
    "This notebook uses the following non-standard python packages:\n",
    "* numpy\n",
    "* pytorch\n",
    "* transformers\n",
    "* treelib\n",
    "* spacy\n",
    "* matplotlib\n",
    "* tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caaea87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import treelib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from itertools import chain\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1919cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add base directory to path\n",
    "if '../' not in os.sys.path:\n",
    "    os.sys.path.insert(0, '../')\n",
    "# import extreme multi label stuff\n",
    "from xmlc.dataset import LabelTreePureRandomDataset, LabelTreeGroupBasedDataset\n",
    "from xmlc.modules import (\n",
    "    MLP, \n",
    "    Attention, \n",
    "    MultiHeadAttention, \n",
    "    LabelAttentionClassifier,\n",
    "    ProbabilityLabelTree\n",
    ")\n",
    "from xmlc.metrics import (\n",
    "    precision, \n",
    "    coverage, \n",
    "    hits\n",
    ")\n",
    "from xmlc.tree_utils import (\n",
    "    index_tree, \n",
    "    yield_tree_levels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577672de",
   "metadata": {},
   "source": [
    "## Paths and Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3494a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/share/gsg_consulting/AttentionXML/data/ops\"\n",
    "fasttext_path = \"/data/share/gsg_consulting/AttentionXML/models/gsg-fasttext\"\n",
    "output_dir = \"../output/ops-fasttext-simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b507d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data hyperparameters\n",
    "max_length = 512\n",
    "num_candidates = 128\n",
    "# model hyperparameters\n",
    "hidden_size=128\n",
    "num_lstm_layers=1\n",
    "dropout=0.5\n",
    "# training hyperparemers\n",
    "lr = 1e-3\n",
    "num_epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a71263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device %s!\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b9861",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d15cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texts(fpath:str):\n",
    "    with open(fpath, \"r\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def load_labels(fpath:str):\n",
    "    return [line.split() for line in load_texts(fpath)]\n",
    "\n",
    "# load training data\n",
    "train_texts = load_texts(os.path.join(data_path, \"train_texts.txt\"))\n",
    "train_labels = load_labels(os.path.join(data_path, \"train_labels.txt\"))\n",
    "assert len(train_texts) == len(train_labels)\n",
    "# load test data\n",
    "test_texts = load_texts(os.path.join(data_path, \"test_texts.txt\"))\n",
    "test_labels = load_labels(os.path.join(data_path, \"test_labels.txt\"))\n",
    "assert len(test_texts) == len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fa2ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique labels: 2703\n"
     ]
    }
   ],
   "source": [
    "# get a list of all unique labels\n",
    "unique_labels = np.unique(tuple(chain(*train_labels)))\n",
    "print(\"# unique labels:\", len(unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3a415",
   "metadata": {},
   "source": [
    "## Build simple label tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b167ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = treelib.Tree()\n",
    "# add root node\n",
    "root = tree.create_node(\"Root\", \"Root\")\n",
    "for label in unique_labels:\n",
    "    tree.create_node(label, label, parent=root)\n",
    "# for now just split by code-type (OPS-3 vs OPS-5)\n",
    "# code_types = {\n",
    "#     '3-': tree.create_node(\"OPS-3\", \"OPS-3\", parent=root),\n",
    "#     '5-': tree.create_node(\"OPS-5\", \"OPS-5\", parent=root)\n",
    "# }\n",
    "# # add labels to corresponding group\n",
    "# for label in unique_labels:\n",
    "#     node = code_types[label[:2]]\n",
    "#     tree.create_node(label, label, parent=node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd709fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth:       1\n",
      "Totel nodes: 2704\n",
      "Inner nodes: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Depth:      \", tree.depth())\n",
    "print(\"Totel nodes:\", len(tree.all_nodes()))\n",
    "print(\"Inner nodes:\", len(tree.all_nodes()) - len(tree.leaves()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b522c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the tree nodes\n",
    "tree = index_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a7533",
   "metadata": {},
   "source": [
    "## Build Training and Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b408c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocab = np.load(os.path.join(fasttext_path, \"vocab.npy\"))\n",
    "embed = np.load(os.path.join(fasttext_path, \"vectors.npy\"))\n",
    "# change special tokens\n",
    "vocab[vocab == \"<SEP>\"] = \"[SEP]\"\n",
    "vocab[vocab == \"<PAD>\"] = \"[PAD]\"\n",
    "vocab[vocab == \"<UNK>\"] = \"[UNK]\"\n",
    "# build token-id map\n",
    "vocab_map = {token.lower(): i for i, token in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26fb4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get german tokenizer\n",
    "from spacy.lang.de import German\n",
    "# build tokenizer parameters\n",
    "prefixes = German.Defaults.prefixes\n",
    "suffixes = German.Defaults.suffixes\n",
    "infixes = German.Defaults.infixes\n",
    "prefix_search = spacy.util.compile_prefix_regex(prefixes).search if prefixes else None\n",
    "suffix_search = spacy.util.compile_suffix_regex(suffixes).search if suffixes else None\n",
    "infix_finditer = spacy.util.compile_infix_regex(infixes).finditer if infixes else None\n",
    "# add tokenizer exception for special tokens\n",
    "exc = German.Defaults.tokenizer_exceptions\n",
    "exc = spacy.util.update_exc(exc, {\n",
    "    '[SEP]': [{spacy.symbols.ORTH: \"[SEP]\"}]\n",
    "})\n",
    "# create tokenizer\n",
    "tokenizer = spacy.tokenizer.Tokenizer(\n",
    "    vocab=spacy.vocab.Vocab(strings=vocab.tolist()),\n",
    "    rules=exc,\n",
    "    prefix_search=prefix_search,\n",
    "    suffix_search=suffix_search,\n",
    "    infix_finditer=infix_finditer,\n",
    "    token_match=German.Defaults.token_match,\n",
    "    url_match=German.Defaults.url_match\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e09b82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_features(texts, max_length=256):\n",
    "    # tokenize all texts and convert tokens to ids\n",
    "    input_ids = []\n",
    "    unk_token_id = vocab_map['[unk]']\n",
    "    pad_token_id = vocab_map['[pad]']\n",
    "    for text in tqdm(texts, \"Tokenizing\"):\n",
    "        # tokenize and convert to ids\n",
    "        doc = tokenizer(text)\n",
    "        ids = [vocab_map.get(str(t).lower(), unk_token_id) for t in doc[:max_length]]\n",
    "        ids = ids + [pad_token_id] * max(max_length - len(ids), 0)\n",
    "        # add to list\n",
    "        input_ids.append(ids)\n",
    "    # convert to tensor\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "    input_mask = (input_ids) != pad_token_id\n",
    "    # return features\n",
    "    return input_ids, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e6917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_input_ids, train_input_mask = build_input_features(train_texts, max_length=max_length)\n",
    "# test_input_ids, test_input_mask = build_input_features(test_texts, max_length=max_length)\n",
    "data = torch.load(\"data.bin\")\n",
    "train_input_ids = data[\"train-input-ids\"]\n",
    "train_input_mask = data[\"train-input-mask\"]\n",
    "test_input_ids = data[\"test-input-ids\"]\n",
    "test_input_mask = data[\"test-input-mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ac5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train and test datasets\n",
    "train_data = LabelTreeGroupBasedDataset(\n",
    "    input_dataset=TensorDataset(train_input_ids, train_input_mask),\n",
    "    tree=tree,\n",
    "    labels=train_labels,\n",
    "    num_candidates=num_candidates,\n",
    ")\n",
    "# observe that the task is much harder when using a Group-based dataset here\n",
    "# as the negative candidates are more similar to the positives\n",
    "# this seems the most fair as it mirrors the setup when no label tree is utilized\n",
    "eval_data = LabelTreePureRandomDataset(\n",
    "    input_dataset=TensorDataset(test_input_ids, test_input_mask),\n",
    "    tree=tree,\n",
    "    labels=test_labels,\n",
    "    num_candidates=num_candidates,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a56a2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fef2f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Basic LSTM Encoder \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "        embed_size:int,\n",
    "        hidden_size:int, \n",
    "        num_layers:int,\n",
    "        vocab_size:int,\n",
    "        padding_idx:int,\n",
    "        emb_init:torch.FloatTensor =None,\n",
    "        dropout:float =0.2\n",
    "    ) -> None:\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        # create embedding\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_size,\n",
    "            padding_idx=padding_idx,\n",
    "            _weight=emb_init if emb_init is not None else None\n",
    "        )\n",
    "        # create lstm encoder\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        # initial hidden and cell states for lstm\n",
    "        self.h0 = nn.Parameter(torch.zeros(num_layers*2, 1, hidden_size))\n",
    "        self.c0 = nn.Parameter(torch.zeros(num_layers*2, 1, hidden_size))\n",
    "                \n",
    "    def forward(self, \n",
    "        input_ids:torch.LongTensor, \n",
    "        input_mask:torch.BoolTensor\n",
    "    ) -> torch.Tensor:\n",
    "        # flatten parameters\n",
    "        self.lstm.flatten_parameters()\n",
    "        # pass through embedding\n",
    "        b, s = input_ids.size()\n",
    "        x = self.embedding.forward(input_ids)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # pack padded sequences\n",
    "        lengths = input_mask.sum(dim=-1).cpu()\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(\n",
    "            input=x, \n",
    "            lengths=lengths, \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        # apply lstm encoder\n",
    "        h0 = self.h0.repeat_interleave(b, dim=1)\n",
    "        c0 = self.c0.repeat_interleave(b, dim=1)\n",
    "        packed_x, _ = self.lstm(packed_x, (h0, c0))\n",
    "        # unpack packed sequences\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            sequence=packed_x, \n",
    "            batch_first=True, \n",
    "            padding_value=0,\n",
    "            total_length=s\n",
    "        )\n",
    "        return F.dropout(x, p=self.dropout, training=self.training)\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \"\"\" Model Combining LSTM-Encoder and a PLT Classifier \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # create encoder\n",
    "        self.enc = LSTMEncoder(\n",
    "            embed_size=500, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            vocab_size=vocab.shape[0], \n",
    "            padding_idx=vocab_map['[pad]'], \n",
    "            emb_init=torch.from_numpy(embed).float(),\n",
    "            dropout=dropout\n",
    "        )\n",
    "        # create hierarchy classifier\n",
    "        self.plt = ProbabilityLabelTree(\n",
    "            tree=tree,\n",
    "            cls_factory=self.classifier_factory\n",
    "        )\n",
    "        \n",
    "    def classifier_factory(self, num_labels) -> nn.Module:\n",
    "        return LabelAttentionClassifier(\n",
    "            hidden_size=2*hidden_size, \n",
    "            num_labels=num_labels,\n",
    "            attention=Attention(dropout=0.5),\n",
    "            #   attention=MultiHeadAttention(\n",
    "            #       embed_dim=2*hidden_size,\n",
    "            #       num_heads=16,\n",
    "            #       dropout=dropout\n",
    "            #   ),\n",
    "            classifier=MLP(2*hidden_size, 128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, candidate_paths, labels=None):\n",
    "        # pass through encoder\n",
    "        x = self.enc(input_ids, input_mask)\n",
    "        # apply classifier\n",
    "        probs = self.plt(x, input_mask, candidate_paths=candidate_paths)\n",
    "        # compute loss if targets given\n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy(probs, labels)\n",
    "            return {'loss': loss, 'logits': probs}\n",
    "        # return logits only\n",
    "        return {'logits': probs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "241b5b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/ndoll/miniconda3/envs/gsg/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# create model and optimizer\n",
    "model = Model()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e893a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Trainable Parameters: 158576625\n"
     ]
    }
   ],
   "source": [
    "n_trainable_params = sum((p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"#Trainable Parameters: %i\" % n_trainable_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580322dc",
   "metadata": {},
   "source": [
    "## Training\n",
    "I'm way too lazy to write this from scratch so lets just use the transformers Trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04f624cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    # unpack predictions and labels\n",
    "    preds, labels = eval_preds\n",
    "    preds = torch.FloatTensor(preds)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    _, preds = torch.topk(preds, k=100, dim=-1)\n",
    "    # compute metrics\n",
    "    return {\n",
    "        \"P@1\": precision(preds, labels, k=1),\n",
    "        \"P@2\": precision(preds, labels, k=2),\n",
    "        \"P@3\": precision(preds, labels, k=3),\n",
    "        \"P@5\": precision(preds, labels, k=5),\n",
    "        \"C@1\": coverage(preds, labels, k=1),\n",
    "        \"C@2\": coverage(preds, labels, k=2),\n",
    "        \"C@3\": coverage(preds, labels, k=3),\n",
    "        \"C@5\": coverage(preds, labels, k=5),\n",
    "        \"H@1\": hits(preds, labels, k=1),\n",
    "        \"H@2\": hits(preds, labels, k=2),\n",
    "        \"H@3\": hits(preds, labels, k=3),\n",
    "        \"H@5\": hits(preds, labels, k=5),\n",
    "    }\n",
    "\n",
    "def collate(batch):\n",
    "    \"\"\" default collate and return as dictionary \"\"\"\n",
    "    return dict(zip(\n",
    "        ('input_ids', 'input_mask', 'candidate_paths', 'labels'),\n",
    "        torch.utils.data._utils.collate.default_collate(batch)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59d89443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=256,\n",
    "    save_steps=5_000,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=250,\n",
    "    eval_steps=250,\n",
    "    evaluation_strategy='steps'\n",
    ")\n",
    "# trainer\n",
    "trainer = Trainer(\n",
    "    optimizers=(optim, None),\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 284607\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8896\n",
      "/home/IAIS/ndoll/miniconda3/envs/gsg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8841' max='8896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8841/8896 57:12 < 00:21, 2.58 it/s, Epoch 7.95/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@2</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>C@1</th>\n",
       "      <th>C@2</th>\n",
       "      <th>C@3</th>\n",
       "      <th>C@5</th>\n",
       "      <th>H@1</th>\n",
       "      <th>H@2</th>\n",
       "      <th>H@3</th>\n",
       "      <th>H@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.070224</td>\n",
       "      <td>0.509147</td>\n",
       "      <td>0.421454</td>\n",
       "      <td>0.351802</td>\n",
       "      <td>0.254996</td>\n",
       "      <td>0.236003</td>\n",
       "      <td>0.390710</td>\n",
       "      <td>0.489208</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.509147</td>\n",
       "      <td>0.550524</td>\n",
       "      <td>0.587795</td>\n",
       "      <td>0.631981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.066000</td>\n",
       "      <td>0.059196</td>\n",
       "      <td>0.545735</td>\n",
       "      <td>0.442077</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.268976</td>\n",
       "      <td>0.252962</td>\n",
       "      <td>0.409828</td>\n",
       "      <td>0.509669</td>\n",
       "      <td>0.623387</td>\n",
       "      <td>0.545735</td>\n",
       "      <td>0.577463</td>\n",
       "      <td>0.612380</td>\n",
       "      <td>0.666627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.587788</td>\n",
       "      <td>0.472725</td>\n",
       "      <td>0.389229</td>\n",
       "      <td>0.282785</td>\n",
       "      <td>0.272455</td>\n",
       "      <td>0.438241</td>\n",
       "      <td>0.541254</td>\n",
       "      <td>0.655390</td>\n",
       "      <td>0.587788</td>\n",
       "      <td>0.617497</td>\n",
       "      <td>0.650329</td>\n",
       "      <td>0.700850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.050987</td>\n",
       "      <td>0.615776</td>\n",
       "      <td>0.491661</td>\n",
       "      <td>0.404878</td>\n",
       "      <td>0.294911</td>\n",
       "      <td>0.285428</td>\n",
       "      <td>0.455795</td>\n",
       "      <td>0.563015</td>\n",
       "      <td>0.683494</td>\n",
       "      <td>0.615776</td>\n",
       "      <td>0.642232</td>\n",
       "      <td>0.676476</td>\n",
       "      <td>0.730904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.046049</td>\n",
       "      <td>0.648325</td>\n",
       "      <td>0.518413</td>\n",
       "      <td>0.424614</td>\n",
       "      <td>0.307180</td>\n",
       "      <td>0.300515</td>\n",
       "      <td>0.480596</td>\n",
       "      <td>0.590459</td>\n",
       "      <td>0.711929</td>\n",
       "      <td>0.648325</td>\n",
       "      <td>0.677177</td>\n",
       "      <td>0.709450</td>\n",
       "      <td>0.761311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.039218</td>\n",
       "      <td>0.678831</td>\n",
       "      <td>0.541174</td>\n",
       "      <td>0.444318</td>\n",
       "      <td>0.319696</td>\n",
       "      <td>0.314656</td>\n",
       "      <td>0.501696</td>\n",
       "      <td>0.617858</td>\n",
       "      <td>0.740937</td>\n",
       "      <td>0.678831</td>\n",
       "      <td>0.706908</td>\n",
       "      <td>0.742372</td>\n",
       "      <td>0.792331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.036695</td>\n",
       "      <td>0.725445</td>\n",
       "      <td>0.570753</td>\n",
       "      <td>0.462944</td>\n",
       "      <td>0.329713</td>\n",
       "      <td>0.336263</td>\n",
       "      <td>0.529118</td>\n",
       "      <td>0.643760</td>\n",
       "      <td>0.764151</td>\n",
       "      <td>0.725445</td>\n",
       "      <td>0.745547</td>\n",
       "      <td>0.773494</td>\n",
       "      <td>0.817156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.033580</td>\n",
       "      <td>0.748729</td>\n",
       "      <td>0.589926</td>\n",
       "      <td>0.476978</td>\n",
       "      <td>0.337049</td>\n",
       "      <td>0.347055</td>\n",
       "      <td>0.546892</td>\n",
       "      <td>0.663275</td>\n",
       "      <td>0.781155</td>\n",
       "      <td>0.748729</td>\n",
       "      <td>0.770592</td>\n",
       "      <td>0.796941</td>\n",
       "      <td>0.835339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.031365</td>\n",
       "      <td>0.768686</td>\n",
       "      <td>0.604633</td>\n",
       "      <td>0.487748</td>\n",
       "      <td>0.342637</td>\n",
       "      <td>0.356306</td>\n",
       "      <td>0.560526</td>\n",
       "      <td>0.678252</td>\n",
       "      <td>0.794106</td>\n",
       "      <td>0.768686</td>\n",
       "      <td>0.789802</td>\n",
       "      <td>0.814936</td>\n",
       "      <td>0.849189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.029470</td>\n",
       "      <td>0.795343</td>\n",
       "      <td>0.624329</td>\n",
       "      <td>0.499168</td>\n",
       "      <td>0.348301</td>\n",
       "      <td>0.368662</td>\n",
       "      <td>0.578785</td>\n",
       "      <td>0.694132</td>\n",
       "      <td>0.807233</td>\n",
       "      <td>0.795343</td>\n",
       "      <td>0.815530</td>\n",
       "      <td>0.834017</td>\n",
       "      <td>0.863226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.027962</td>\n",
       "      <td>0.810026</td>\n",
       "      <td>0.633571</td>\n",
       "      <td>0.505773</td>\n",
       "      <td>0.351998</td>\n",
       "      <td>0.375468</td>\n",
       "      <td>0.587353</td>\n",
       "      <td>0.703317</td>\n",
       "      <td>0.815801</td>\n",
       "      <td>0.810026</td>\n",
       "      <td>0.827602</td>\n",
       "      <td>0.845053</td>\n",
       "      <td>0.872389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.823188</td>\n",
       "      <td>0.643930</td>\n",
       "      <td>0.513249</td>\n",
       "      <td>0.355733</td>\n",
       "      <td>0.381569</td>\n",
       "      <td>0.596956</td>\n",
       "      <td>0.713713</td>\n",
       "      <td>0.824457</td>\n",
       "      <td>0.823188</td>\n",
       "      <td>0.841133</td>\n",
       "      <td>0.857544</td>\n",
       "      <td>0.881645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>0.836968</td>\n",
       "      <td>0.650606</td>\n",
       "      <td>0.516686</td>\n",
       "      <td>0.358517</td>\n",
       "      <td>0.387956</td>\n",
       "      <td>0.603145</td>\n",
       "      <td>0.718493</td>\n",
       "      <td>0.830911</td>\n",
       "      <td>0.836968</td>\n",
       "      <td>0.849854</td>\n",
       "      <td>0.863286</td>\n",
       "      <td>0.888546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.024387</td>\n",
       "      <td>0.844999</td>\n",
       "      <td>0.655405</td>\n",
       "      <td>0.521644</td>\n",
       "      <td>0.361473</td>\n",
       "      <td>0.391679</td>\n",
       "      <td>0.607594</td>\n",
       "      <td>0.725387</td>\n",
       "      <td>0.837760</td>\n",
       "      <td>0.844999</td>\n",
       "      <td>0.856123</td>\n",
       "      <td>0.871570</td>\n",
       "      <td>0.895871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.852839</td>\n",
       "      <td>0.663079</td>\n",
       "      <td>0.525905</td>\n",
       "      <td>0.363402</td>\n",
       "      <td>0.395313</td>\n",
       "      <td>0.614709</td>\n",
       "      <td>0.731311</td>\n",
       "      <td>0.842232</td>\n",
       "      <td>0.852839</td>\n",
       "      <td>0.866147</td>\n",
       "      <td>0.878688</td>\n",
       "      <td>0.900652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.022737</td>\n",
       "      <td>0.855690</td>\n",
       "      <td>0.666405</td>\n",
       "      <td>0.528930</td>\n",
       "      <td>0.364771</td>\n",
       "      <td>0.396635</td>\n",
       "      <td>0.617792</td>\n",
       "      <td>0.735518</td>\n",
       "      <td>0.845403</td>\n",
       "      <td>0.855690</td>\n",
       "      <td>0.870492</td>\n",
       "      <td>0.883743</td>\n",
       "      <td>0.904044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.669423</td>\n",
       "      <td>0.531148</td>\n",
       "      <td>0.366795</td>\n",
       "      <td>0.401722</td>\n",
       "      <td>0.620589</td>\n",
       "      <td>0.738602</td>\n",
       "      <td>0.850095</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.874434</td>\n",
       "      <td>0.887448</td>\n",
       "      <td>0.909061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.021504</td>\n",
       "      <td>0.870516</td>\n",
       "      <td>0.673509</td>\n",
       "      <td>0.534442</td>\n",
       "      <td>0.368325</td>\n",
       "      <td>0.403506</td>\n",
       "      <td>0.624378</td>\n",
       "      <td>0.743183</td>\n",
       "      <td>0.853641</td>\n",
       "      <td>0.870516</td>\n",
       "      <td>0.879772</td>\n",
       "      <td>0.892953</td>\n",
       "      <td>0.912853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.020822</td>\n",
       "      <td>0.875742</td>\n",
       "      <td>0.676954</td>\n",
       "      <td>0.536675</td>\n",
       "      <td>0.369598</td>\n",
       "      <td>0.405929</td>\n",
       "      <td>0.627571</td>\n",
       "      <td>0.746289</td>\n",
       "      <td>0.856592</td>\n",
       "      <td>0.875742</td>\n",
       "      <td>0.884272</td>\n",
       "      <td>0.896684</td>\n",
       "      <td>0.916009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.020452</td>\n",
       "      <td>0.879116</td>\n",
       "      <td>0.679425</td>\n",
       "      <td>0.538275</td>\n",
       "      <td>0.370178</td>\n",
       "      <td>0.407493</td>\n",
       "      <td>0.629862</td>\n",
       "      <td>0.748513</td>\n",
       "      <td>0.857936</td>\n",
       "      <td>0.879116</td>\n",
       "      <td>0.887499</td>\n",
       "      <td>0.899357</td>\n",
       "      <td>0.917446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.020402</td>\n",
       "      <td>0.881207</td>\n",
       "      <td>0.682015</td>\n",
       "      <td>0.539542</td>\n",
       "      <td>0.371214</td>\n",
       "      <td>0.408462</td>\n",
       "      <td>0.632263</td>\n",
       "      <td>0.750275</td>\n",
       "      <td>0.860337</td>\n",
       "      <td>0.881207</td>\n",
       "      <td>0.890882</td>\n",
       "      <td>0.901474</td>\n",
       "      <td>0.920013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.019628</td>\n",
       "      <td>0.884105</td>\n",
       "      <td>0.682870</td>\n",
       "      <td>0.540144</td>\n",
       "      <td>0.372088</td>\n",
       "      <td>0.409806</td>\n",
       "      <td>0.633056</td>\n",
       "      <td>0.751112</td>\n",
       "      <td>0.862363</td>\n",
       "      <td>0.884105</td>\n",
       "      <td>0.891999</td>\n",
       "      <td>0.902480</td>\n",
       "      <td>0.922180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.019438</td>\n",
       "      <td>0.885198</td>\n",
       "      <td>0.685317</td>\n",
       "      <td>0.542789</td>\n",
       "      <td>0.372934</td>\n",
       "      <td>0.410312</td>\n",
       "      <td>0.635324</td>\n",
       "      <td>0.754791</td>\n",
       "      <td>0.864323</td>\n",
       "      <td>0.885198</td>\n",
       "      <td>0.895196</td>\n",
       "      <td>0.906899</td>\n",
       "      <td>0.924276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.018994</td>\n",
       "      <td>0.893134</td>\n",
       "      <td>0.688810</td>\n",
       "      <td>0.544041</td>\n",
       "      <td>0.373419</td>\n",
       "      <td>0.413991</td>\n",
       "      <td>0.638562</td>\n",
       "      <td>0.756531</td>\n",
       "      <td>0.865446</td>\n",
       "      <td>0.893134</td>\n",
       "      <td>0.899758</td>\n",
       "      <td>0.908990</td>\n",
       "      <td>0.925478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.018735</td>\n",
       "      <td>0.893942</td>\n",
       "      <td>0.689332</td>\n",
       "      <td>0.545640</td>\n",
       "      <td>0.374531</td>\n",
       "      <td>0.414365</td>\n",
       "      <td>0.639047</td>\n",
       "      <td>0.758755</td>\n",
       "      <td>0.868023</td>\n",
       "      <td>0.893942</td>\n",
       "      <td>0.900441</td>\n",
       "      <td>0.911663</td>\n",
       "      <td>0.928233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.018907</td>\n",
       "      <td>0.892373</td>\n",
       "      <td>0.689285</td>\n",
       "      <td>0.545450</td>\n",
       "      <td>0.374711</td>\n",
       "      <td>0.413638</td>\n",
       "      <td>0.639003</td>\n",
       "      <td>0.758491</td>\n",
       "      <td>0.868442</td>\n",
       "      <td>0.892373</td>\n",
       "      <td>0.900379</td>\n",
       "      <td>0.911345</td>\n",
       "      <td>0.928681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.018491</td>\n",
       "      <td>0.894939</td>\n",
       "      <td>0.691684</td>\n",
       "      <td>0.546226</td>\n",
       "      <td>0.375690</td>\n",
       "      <td>0.414828</td>\n",
       "      <td>0.641227</td>\n",
       "      <td>0.759570</td>\n",
       "      <td>0.870711</td>\n",
       "      <td>0.894939</td>\n",
       "      <td>0.903513</td>\n",
       "      <td>0.912642</td>\n",
       "      <td>0.931107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.018128</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.692207</td>\n",
       "      <td>0.547383</td>\n",
       "      <td>0.376013</td>\n",
       "      <td>0.416501</td>\n",
       "      <td>0.641712</td>\n",
       "      <td>0.761178</td>\n",
       "      <td>0.871459</td>\n",
       "      <td>0.898551</td>\n",
       "      <td>0.904196</td>\n",
       "      <td>0.914574</td>\n",
       "      <td>0.931908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.018101</td>\n",
       "      <td>0.901022</td>\n",
       "      <td>0.694108</td>\n",
       "      <td>0.548254</td>\n",
       "      <td>0.376669</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.643474</td>\n",
       "      <td>0.762389</td>\n",
       "      <td>0.872979</td>\n",
       "      <td>0.901022</td>\n",
       "      <td>0.906679</td>\n",
       "      <td>0.916029</td>\n",
       "      <td>0.933533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.018600</td>\n",
       "      <td>0.017783</td>\n",
       "      <td>0.901782</td>\n",
       "      <td>0.694559</td>\n",
       "      <td>0.548523</td>\n",
       "      <td>0.377097</td>\n",
       "      <td>0.417999</td>\n",
       "      <td>0.643892</td>\n",
       "      <td>0.762764</td>\n",
       "      <td>0.873970</td>\n",
       "      <td>0.901782</td>\n",
       "      <td>0.907268</td>\n",
       "      <td>0.916479</td>\n",
       "      <td>0.934593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.017701</td>\n",
       "      <td>0.900356</td>\n",
       "      <td>0.695272</td>\n",
       "      <td>0.549252</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>0.417338</td>\n",
       "      <td>0.644553</td>\n",
       "      <td>0.763777</td>\n",
       "      <td>0.873860</td>\n",
       "      <td>0.900356</td>\n",
       "      <td>0.908199</td>\n",
       "      <td>0.917697</td>\n",
       "      <td>0.934475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.017577</td>\n",
       "      <td>0.904443</td>\n",
       "      <td>0.696626</td>\n",
       "      <td>0.549758</td>\n",
       "      <td>0.377401</td>\n",
       "      <td>0.419233</td>\n",
       "      <td>0.645809</td>\n",
       "      <td>0.764482</td>\n",
       "      <td>0.874675</td>\n",
       "      <td>0.904443</td>\n",
       "      <td>0.909968</td>\n",
       "      <td>0.918543</td>\n",
       "      <td>0.935346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.017580</td>\n",
       "      <td>0.903017</td>\n",
       "      <td>0.695296</td>\n",
       "      <td>0.549410</td>\n",
       "      <td>0.377553</td>\n",
       "      <td>0.418572</td>\n",
       "      <td>0.644575</td>\n",
       "      <td>0.763997</td>\n",
       "      <td>0.875028</td>\n",
       "      <td>0.903017</td>\n",
       "      <td>0.908230</td>\n",
       "      <td>0.917961</td>\n",
       "      <td>0.935723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.017497</td>\n",
       "      <td>0.905678</td>\n",
       "      <td>0.696769</td>\n",
       "      <td>0.551089</td>\n",
       "      <td>0.377505</td>\n",
       "      <td>0.419805</td>\n",
       "      <td>0.645941</td>\n",
       "      <td>0.766332</td>\n",
       "      <td>0.874917</td>\n",
       "      <td>0.905678</td>\n",
       "      <td>0.910155</td>\n",
       "      <td>0.920766</td>\n",
       "      <td>0.935605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.017471</td>\n",
       "      <td>0.905013</td>\n",
       "      <td>0.697672</td>\n",
       "      <td>0.550281</td>\n",
       "      <td>0.377771</td>\n",
       "      <td>0.419497</td>\n",
       "      <td>0.646778</td>\n",
       "      <td>0.765209</td>\n",
       "      <td>0.875534</td>\n",
       "      <td>0.905013</td>\n",
       "      <td>0.911334</td>\n",
       "      <td>0.919417</td>\n",
       "      <td>0.936265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to ../output/ops-fasttext-simple/checkpoint-5000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/home/IAIS/ndoll/miniconda3/envs/gsg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 21045\n",
      "  Batch size = 512\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final model state dict to disk\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"final-model.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a4082",
   "metadata": {},
   "source": [
    "## Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed386c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only train and test logs\n",
    "train_logs = [log for log in trainer.state.log_history if 'loss' in log]\n",
    "eval_logs = [log for log in trainer.state.log_history if 'eval_loss' in log]\n",
    "# gather values from logs\n",
    "train_metrics = {\n",
    "    key: [log[key] for log in train_logs]\n",
    "    for key in ['step', 'loss']\n",
    "}\n",
    "eval_logs = {\n",
    "    key: [log[key] for log in eval_logs]\n",
    "    for key in [\n",
    "        'step', \n",
    "        'eval_loss', \n",
    "        'eval_P@1', \n",
    "        'eval_P@2', \n",
    "        'eval_P@3', \n",
    "        'eval_P@5',\n",
    "        'eval_C@1', \n",
    "        'eval_C@2', \n",
    "        'eval_C@3', \n",
    "        'eval_C@5',\n",
    "        'eval_H@1', \n",
    "        'eval_H@2', \n",
    "        'eval_H@3', \n",
    "        'eval_H@5',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_loss, ax_p, ax_c, ax_h) = plt.subplots(4, 1, figsize=(12, 20), sharex=True)\n",
    "# plot losses\n",
    "ax_loss.plot(train_metrics['step'], train_metrics['loss'], label=\"train\")\n",
    "ax_loss.plot(eval_logs['step'], eval_logs['eval_loss'], label=\"eval\")\n",
    "ax_loss.set(\n",
    "    title=\"Train and Test Loss\",\n",
    "    ylabel=\"Loss\"\n",
    ")\n",
    "ax_loss.legend()\n",
    "ax_loss.grid()\n",
    "# plot precisions\n",
    "ax_p.plot(eval_logs['step'], eval_logs['eval_P@1'], label=\"$k=1$\")\n",
    "ax_p.plot(eval_logs['step'], eval_logs['eval_P@2'], label=\"$k=2$\")\n",
    "ax_p.plot(eval_logs['step'], eval_logs['eval_P@3'], label=\"$k=3$\")\n",
    "ax_p.plot(eval_logs['step'], eval_logs['eval_P@5'], label=\"$k=5$\")\n",
    "ax_p.set(\n",
    "    title=\"Precision@k\",\n",
    "    ylabel=\"Precision\"\n",
    ")\n",
    "ax_p.legend()\n",
    "ax_p.grid()\n",
    "# plot coverages\n",
    "ax_c.plot(eval_logs['step'], eval_logs['eval_C@1'], label=\"$k=1$\")\n",
    "ax_c.plot(eval_logs['step'], eval_logs['eval_C@2'], label=\"$k=2$\")\n",
    "ax_c.plot(eval_logs['step'], eval_logs['eval_C@3'], label=\"$k=3$\")\n",
    "ax_c.plot(eval_logs['step'], eval_logs['eval_C@5'], label=\"$k=5$\")\n",
    "ax_c.set(\n",
    "    title=\"Coverage@k\",\n",
    "    ylabel=\"Coverage\"\n",
    ")\n",
    "ax_c.legend()\n",
    "ax_c.grid()\n",
    "# plot hits\n",
    "ax_h.plot(eval_logs['step'], eval_logs['eval_H@1'], label=\"$k=1$\")\n",
    "ax_h.plot(eval_logs['step'], eval_logs['eval_H@2'], label=\"$k=2$\")\n",
    "ax_h.plot(eval_logs['step'], eval_logs['eval_H@3'], label=\"$k=3$\")\n",
    "ax_h.plot(eval_logs['step'], eval_logs['eval_H@5'], label=\"$k=5$\")\n",
    "ax_h.set(\n",
    "    title=\"Hits@k\",\n",
    "    ylabel=\"Hits\",\n",
    "    xlabel=\"Global Step\"\n",
    ")\n",
    "ax_h.legend()\n",
    "ax_h.grid()\n",
    "# save figure\n",
    "fig.savefig(os.path.join(output_dir, \"metrics.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d691e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
