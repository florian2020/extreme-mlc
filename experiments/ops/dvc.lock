schema: '2.0'
stages:
  prepare:
    cmd: python ../../src/prepare.py --code-type ops_codes --train-source /data/share/gsg_consulting/AttentionXML/old_data/ops-combined/train.csv
      --test-source /data/share/gsg_consulting/AttentionXML/old_data/ops-combined/test.csv
      --min-label-freq 10 --max-num-labels -1 --output-dir data/prepared
    params:
      params.yaml:
        prepare.MaxNumberLabels: -1
        prepare.MinLabelFrequency: 10
    outs:
    - path: data/prepared/hospitals.txt
      md5: 2ce0ad1321eedd3f9665cebaaeee6b00
      size: 109
    - path: data/prepared/labels.txt
      md5: 0a0718600ccc9d857691c35146d9df0f
      size: 16385
    - path: data/prepared/metrics.json
      md5: fb849cdfe84c9f23a1ef241030010a35
      size: 299
    - path: data/prepared/test_labels.txt
      md5: f84bd48450526bffa9682de977acab92
      size: 461393
    - path: data/prepared/test_texts.txt
      md5: 0cb3ef8ac92e43d973556c2be5ec2f5f
      size: 121913194
    - path: data/prepared/train_labels.txt
      md5: 2f79c5ca1aaf5b897a3ea4d4fd3e3eb5
      size: 1856135
    - path: data/prepared/train_texts.txt
      md5: 4cf7ca5107f77ace0d1927ff0b9fbe45
      size: 500248518
  preprocess:
    cmd: python ../../src/preprocess.py --train-texts data/prepared/train_texts.txt
      --test-texts data/prepared/test_texts.txt --tokenizer Spacy --max-length 256
      --pretrained-vocab ../../pretrained/gsg-fasttext/vocab.npy --pretrained-embed
      ../../pretrained/gsg-fasttext/vectors.npy --output-dir data/preprocessed
    deps:
    - path: data/prepared/test_texts.txt
      md5: 0cb3ef8ac92e43d973556c2be5ec2f5f
      size: 121913194
    - path: data/prepared/train_texts.txt
      md5: 4cf7ca5107f77ace0d1927ff0b9fbe45
      size: 500248518
    params:
      params.yaml:
        preprocess.embedding: gsg-fasttext
        preprocess.max_length: 256
        preprocess.tokenizer: Spacy
    outs:
    - path: data/preprocessed/metrics.json
      md5: 3682fa404939aa248067b4c20d8dcdc8
      size: 121
    - path: data/preprocessed/test_input_ids.pkl
      md5: f5efcaaf88d01acd26ac4c28248f4d20
      size: 65503992
    - path: data/preprocessed/train_input_ids.pkl
      md5: 5b51afce2bd3db1d8a0f383f89ccfb56
      size: 262675192
    - path: data/preprocessed/vectors.npy
      md5: ce06f0684b096a647f5a8adb52b59312
      size: 343332128
    - path: data/preprocessed/vocab.json
      md5: 4c87ec9c415992a0503e9fb2a1b714c4
      size: 1978483
  build_label_tree:
    cmd: PYTHONPATH='../../' python ../../src/build_label_tree.py --labels-file data/prepared/labels.txt
      --group-id-chars 0 --output-file model/label_tree.pkl
    deps:
    - path: data/prepared/labels.txt
      md5: 0a0718600ccc9d857691c35146d9df0f
      size: 16385
    params:
      params.yaml:
        label_tree.group_id_chars: 0
    outs:
    - path: model/label_tree.pkl
      md5: fd2d48d39b55c8a34b8c2471998e8b84
      size: 203729
  train:
    cmd: PYTHONPATH=../../ python ../../src/train.py --train-input-ids data/preprocessed/train_input_ids.pkl
      --test-input-ids data/preprocessed/test_input_ids.pkl --train-labels data/prepared/train_labels.txt
      --test-labels data/prepared/test_labels.txt --vocab data/preprocessed/vocab.json
      --embed data/preprocessed/vectors.npy --label-tree model/label_tree.pkl --output-dir
      model
    deps:
    - path: data/prepared/test_labels.txt
      md5: f84bd48450526bffa9682de977acab92
      size: 461393
    - path: data/prepared/train_labels.txt
      md5: 2f79c5ca1aaf5b897a3ea4d4fd3e3eb5
      size: 1856135
    - path: data/preprocessed/test_input_ids.pkl
      md5: f5efcaaf88d01acd26ac4c28248f4d20
      size: 65503992
    - path: data/preprocessed/train_input_ids.pkl
      md5: 5b51afce2bd3db1d8a0f383f89ccfb56
      size: 262675192
    - path: data/preprocessed/vectors.npy
      md5: ce06f0684b096a647f5a8adb52b59312
      size: 343332128
    - path: data/preprocessed/vocab.json
      md5: 4c87ec9c415992a0503e9fb2a1b714c4
      size: 1978483
    - path: model/label_tree.pkl
      md5: fd2d48d39b55c8a34b8c2471998e8b84
      size: 203729
    params:
      params.yaml:
        model.attention.type: softmax-attention
        model.dropout: 0.5
        model.encoder.hidden_size: 256
        model.encoder.num_layers: 1
        model.mlp.activation: relu
        model.mlp.bias: true
        model.mlp.hidden_layers:
        - 256
        trainer.eval_batch_size: 256
        trainer.eval_interval: 250
        trainer.num_candidates:
        trainer.num_steps: 10000
        trainer.regime: levelwise
        trainer.topk: 1
        trainer.train_batch_size: 128
    outs:
    - path: model/metrics.json
      md5: 09f381a94d0e76c01c932d52b5e9fde8
      size: 375
    - path: model/model.bin
      md5: d109c2565e79a369e8bd565f81b5bbeb
      size: 182346313
